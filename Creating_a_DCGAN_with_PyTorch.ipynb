{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPidJbYyNiX6ljbKMjGtF2R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@francescofranco_39234/creating-a-dcgan-with-pytorch-98af026d3ba0)"
      ],
      "metadata": {
        "id": "pQj6KD-sXAIn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MMv2GHmjUXBz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import uuid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurable variables\n",
        "NUM_EPOCHS = 50\n",
        "NOISE_DIMENSION = 50\n",
        "BATCH_SIZE = 128\n",
        "TRAIN_ON_GPU = True\n",
        "UNIQUE_RUN_ID = str(uuid.uuid4())\n",
        "PRINT_STATS_AFTER_BATCH = 50\n",
        "OPTIMIZER_LR = 0.0002\n",
        "OPTIMIZER_BETAS = (0.5, 0.999)"
      ],
      "metadata": {
        "id": "ZYQv-mdAXkK0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Speed ups\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "torch.autograd.profiler.profile(False)\n",
        "torch.autograd.profiler.emit_nvtx(False)\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "-DpTKVH3XlR4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Generator"
      ],
      "metadata": {
        "id": "m7ozZUf9Xouh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"\n",
        "    DCGan Generator\n",
        "  \"\"\"\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "    num_feature_maps = 64\n",
        "    self.layers = nn.Sequential(\n",
        "      # First upsampling block\n",
        "      nn.ConvTranspose2d(NOISE_DIMENSION, num_feature_maps * 8, 4, 1, 0, bias=False),\n",
        "      nn.BatchNorm2d(num_feature_maps * 8),\n",
        "      nn.ReLU(),\n",
        "      # Second upsampling block\n",
        "      nn.ConvTranspose2d(num_feature_maps * 8, num_feature_maps * 4, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(num_feature_maps * 4),\n",
        "      nn.ReLU(),\n",
        "      # Third upsampling block\n",
        "      nn.ConvTranspose2d(num_feature_maps * 4, num_feature_maps * 2, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(num_feature_maps * 2),\n",
        "      nn.ReLU(),\n",
        "      # Fourth upsampling block\n",
        "      nn.ConvTranspose2d(num_feature_maps * 2, num_feature_maps, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(num_feature_maps),\n",
        "      nn.ReLU(),\n",
        "      # Fifth upsampling block: note Tanh\n",
        "      nn.ConvTranspose2d(num_feature_maps, 1, 1, 1, 2, bias=False),\n",
        "      nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Forward pass\"\"\"\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "CiUmEKSAXmlw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Discriminator"
      ],
      "metadata": {
        "id": "OF2vNPG7XwEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  \"\"\"\n",
        "    DCGan Discriminator\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    num_feature_maps = 64\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Conv2d(1, num_feature_maps, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(num_feature_maps * 1),\n",
        "      nn.LeakyReLU(0.2),\n",
        "      nn.Conv2d(num_feature_maps, num_feature_maps * 2, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(num_feature_maps * 2),\n",
        "      nn.LeakyReLU(0.2),\n",
        "      nn.Conv2d(num_feature_maps * 2, num_feature_maps * 4, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(num_feature_maps * 4),\n",
        "      nn.LeakyReLU(0.2),\n",
        "      nn.Conv2d(num_feature_maps * 4, 1, 4, 2, 1, bias=False),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(1, 1),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Forward pass\"\"\"\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "GOBsDzILXvKZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the PyTorch device"
      ],
      "metadata": {
        "id": "PgBxIe5BX7f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "  \"\"\" Retrieve device based on settings and availability. \"\"\"\n",
        "  return torch.device(\"cuda:0\" if torch.cuda.is_available() and TRAIN_ON_GPU else \"cpu\")"
      ],
      "metadata": {
        "id": "31sSzZN9XycC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making run directory and generating images"
      ],
      "metadata": {
        "id": "VIh-Vb8wX9gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_directory_for_run():\n",
        "  \"\"\" Make a directory for this training run. \"\"\"\n",
        "  print(f'Preparing training run {UNIQUE_RUN_ID}')\n",
        "  if not os.path.exists('./runs'):\n",
        "    os.mkdir('./runs')\n",
        "  os.mkdir(f'./runs/{UNIQUE_RUN_ID}')"
      ],
      "metadata": {
        "id": "zwyIHHeZX8n5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(generator, epoch = 0, batch = 0, device=get_device()):\n",
        "  \"\"\" Generate subplots with generated examples. \"\"\"\n",
        "  images = []\n",
        "  noise = generate_noise(BATCH_SIZE, device=device)\n",
        "  generator.eval()\n",
        "  images = generator(noise)\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  for i in range(16):\n",
        "    # Get image\n",
        "    image = images[i]\n",
        "    # Convert image back onto CPU and reshape\n",
        "    image = image.cpu().detach().numpy()\n",
        "    image = np.reshape(image, (28, 28))\n",
        "    # Plot\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "  if not os.path.exists(f'./runs/{UNIQUE_RUN_ID}/images'):\n",
        "    os.mkdir(f'./runs/{UNIQUE_RUN_ID}/images')\n",
        "  plt.savefig(f'./runs/{UNIQUE_RUN_ID}/images/epoch{epoch}_batch{batch}.jpg')"
      ],
      "metadata": {
        "id": "IT6eoikiX_Jh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functionality for saving models & printing progress"
      ],
      "metadata": {
        "id": "zdrQF17gYCwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_models(generator, discriminator, epoch):\n",
        "  \"\"\" Save models at specific point in time. \"\"\"\n",
        "  torch.save(generator.state_dict(), f'./runs/{UNIQUE_RUN_ID}/generator_{epoch}.pth')\n",
        "  torch.save(discriminator.state_dict(), f'./runs/{UNIQUE_RUN_ID}/discriminator_{epoch}.pth')"
      ],
      "metadata": {
        "id": "36KalMw_YAmk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_training_progress(batch, generator_loss, discriminator_loss):\n",
        "  \"\"\" Print training progress. \"\"\"\n",
        "  print('Losses after mini-batch %5d: generator %e, discriminator %e' %\n",
        "        (batch, generator_loss, discriminator_loss))"
      ],
      "metadata": {
        "id": "rjfMlQ8iYD_5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the dataset"
      ],
      "metadata": {
        "id": "2GZyhOsNYF_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset():\n",
        "  \"\"\" Prepare dataset through DataLoader \"\"\"\n",
        "  # Prepare MNIST dataset\n",
        "  dataset = MNIST(os.getcwd(), download=True, train=True, transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "  ]))\n",
        "  # Batch and shuffle data with DataLoader\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "  # Return dataset through DataLoader\n",
        "  return trainloader"
      ],
      "metadata": {
        "id": "SdGnvZgmYE6B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weight initializer function"
      ],
      "metadata": {
        "id": "ZAOgQ9u9YIkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "  \"\"\" Normal weight initialization as suggested for DCGANs \"\"\"\n",
        "  classname = m.__class__.__name__\n",
        "  if classname.find('Conv') != -1:\n",
        "      nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "  elif classname.find('BatchNorm') != -1:\n",
        "      nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "      nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "0iiWkuKmYHSX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing models, loss and optimizers"
      ],
      "metadata": {
        "id": "vuoEL8ZcYKlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_models(device = get_device()):\n",
        "  \"\"\" Initialize Generator and Discriminator models \"\"\"\n",
        "  generator = Generator()\n",
        "  discriminator = Discriminator()\n",
        "  # Perform proper weight initialization\n",
        "  generator.apply(weights_init)\n",
        "  discriminator.apply(weights_init)\n",
        "  # Move models to specific device\n",
        "  generator.to(device)\n",
        "  discriminator.to(device)\n",
        "  # Return models\n",
        "  return generator, discriminator\n",
        "\n",
        "\n",
        "def initialize_loss():\n",
        "  \"\"\" Initialize loss function. \"\"\"\n",
        "  return nn.BCELoss()\n",
        "\n",
        "\n",
        "def initialize_optimizers(generator, discriminator):\n",
        "  \"\"\" Initialize optimizers for Generator and Discriminator. \"\"\"\n",
        "  generator_optimizer = torch.optim.AdamW(generator.parameters(), lr=OPTIMIZER_LR,betas=OPTIMIZER_BETAS)\n",
        "  discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=OPTIMIZER_LR,betas=OPTIMIZER_BETAS)\n",
        "  return generator_optimizer, discriminator_optimizer"
      ],
      "metadata": {
        "id": "bcyI3YMLYJpW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating noise"
      ],
      "metadata": {
        "id": "p1k3QMcHYNOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_noise(number_of_images = 1, noise_dimension = NOISE_DIMENSION, device=None):\n",
        "  \"\"\" Generate noise for number_of_images images, with a specific noise_dimension \"\"\"\n",
        "  return torch.randn(number_of_images, noise_dimension, 1, 1, device=device)"
      ],
      "metadata": {
        "id": "mOYr2a1DYMDv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficiently zero-ing gradients"
      ],
      "metadata": {
        "id": "rSedBBpdYPdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def efficient_zero_grad(model):\n",
        "  \"\"\"\n",
        "    Apply zero_grad more efficiently\n",
        "    Source: https://betterprogramming.pub/how-to-make-your-pytorch-code-run-faster-93079f3c1f7b\n",
        "  \"\"\"\n",
        "  for param in model.parameters():\n",
        "    param.grad = None"
      ],
      "metadata": {
        "id": "_qF0RN9cYOcP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward and backward passes"
      ],
      "metadata": {
        "id": "SD0XIxGEYR74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_and_backward(model, data, loss_function, targets):\n",
        "  \"\"\"\n",
        "    Perform forward and backward pass in a generic way. Returns loss value.\n",
        "  \"\"\"\n",
        "  outputs = model(data)\n",
        "  error = loss_function(outputs, targets)\n",
        "  error.backward()\n",
        "  return error.item()"
      ],
      "metadata": {
        "id": "vGR8GLBLYQ7c"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining the passes into a training step"
      ],
      "metadata": {
        "id": "oKQzZeoEYUQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_train_step(generator, discriminator, real_data, \\\n",
        "  loss_function, generator_optimizer, discriminator_optimizer, device = get_device()):\n",
        "  \"\"\" Perform a single training step. \"\"\"\n",
        "\n",
        "  # 1. PREPARATION\n",
        "  # Set real and fake labels.\n",
        "  real_label, fake_label = 1.0, 0.0\n",
        "  # Get images on CPU or GPU as configured and available\n",
        "  # Also set 'actual batch size', whih can be smaller than BATCH_SIZE\n",
        "  # in some cases.\n",
        "  real_images = real_data[0].to(device)\n",
        "  actual_batch_size = real_images.size(0)\n",
        "  label = torch.full((actual_batch_size,1), real_label, device=device)\n",
        "\n",
        "  # 2. TRAINING THE DISCRIMINATOR\n",
        "  # Zero the gradients for discriminator\n",
        "  efficient_zero_grad(discriminator)\n",
        "  # Forward + backward on real iamges\n",
        "  error_real_images = forward_and_backward(discriminator, real_images, \\\n",
        "    loss_function, label)\n",
        "  # Forward + backward on generated images\n",
        "  noise = generate_noise(actual_batch_size, device=device)\n",
        "  generated_images = generator(noise)\n",
        "  label.fill_(fake_label)\n",
        "  error_generated_images =forward_and_backward(discriminator, \\\n",
        "    generated_images.detach(), loss_function, label)\n",
        "  # Optim for discriminator\n",
        "  discriminator_optimizer.step()\n",
        "\n",
        "  # 3. TRAINING THE GENERATOR\n",
        "  # Forward + backward + optim for generator, including zero grad\n",
        "  efficient_zero_grad(generator)\n",
        "  label.fill_(real_label)\n",
        "  error_generator = forward_and_backward(discriminator, generated_images, loss_function, label)\n",
        "  generator_optimizer.step()\n",
        "\n",
        "  # 4. COMPUTING RESULTS\n",
        "  # Compute loss values in floats for discriminator, which is joint loss.\n",
        "  error_discriminator = error_real_images + error_generated_images\n",
        "  # Return generator and discriminator loss so that it can be printed.\n",
        "  return error_generator, error_discriminator"
      ],
      "metadata": {
        "id": "Ru4VLaOTYTSm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining training steps into epochs"
      ],
      "metadata": {
        "id": "NFf6WJnhYXzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_epoch(dataloader, generator, discriminator, loss_function, \\\n",
        "    generator_optimizer, discriminator_optimizer, epoch):\n",
        "  \"\"\" Perform a single epoch. \"\"\"\n",
        "  for batch_no, real_data in enumerate(dataloader, 0):\n",
        "    # Perform training step\n",
        "    generator_loss_val, discriminator_loss_val = perform_train_step(generator, \\\n",
        "      discriminator, real_data, loss_function, \\\n",
        "      generator_optimizer, discriminator_optimizer)\n",
        "    # Print statistics and generate image after every n-th batch\n",
        "    if batch_no % PRINT_STATS_AFTER_BATCH == 0:\n",
        "      print_training_progress(batch_no, generator_loss_val, discriminator_loss_val)\n",
        "      generate_image(generator, epoch, batch_no)\n",
        "  # Save models on epoch completion.\n",
        "  save_models(generator, discriminator, epoch)\n",
        "  # Clear memory after every epoch\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "O86iLde1YW0q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining epochs into a DCGAN"
      ],
      "metadata": {
        "id": "XvbnGVy-YZ3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dcgan():\n",
        "  \"\"\" Train the DCGAN. \"\"\"\n",
        "  # Make directory for unique run\n",
        "  make_directory_for_run()\n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  # Get prepared dataset\n",
        "  dataloader = prepare_dataset()\n",
        "  # Initialize models\n",
        "  generator, discriminator = initialize_models()\n",
        "  # Initialize loss and optimizers\n",
        "  loss_function = initialize_loss()\n",
        "  generator_optimizer, discriminator_optimizer = initialize_optimizers(generator, discriminator)\n",
        "  # Train the model\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print(f'Starting epoch {epoch}...')\n",
        "    perform_epoch(dataloader, generator, discriminator, loss_function, \\\n",
        "      generator_optimizer, discriminator_optimizer, epoch)\n",
        "  # Finished :-)\n",
        "  print(f'Finished unique run {UNIQUE_RUN_ID}')"
      ],
      "metadata": {
        "id": "gPfKswrzYY9l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing and starting GAN training"
      ],
      "metadata": {
        "id": "mAy_dejcYeh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  train_dcgan()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XaxGJ5iYdby",
        "outputId": "289e8065-06be-4456-bb8a-d2103a7c7d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training run 71b43d7d-1525-41d8-bb85-6df6b65697d5\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 63898144.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 76042494.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27205747.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16035798.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0...\n",
            "Losses after mini-batch     0: generator 1.357544e+00, discriminator 1.705213e+00\n",
            "Losses after mini-batch    50: generator 4.083583e+00, discriminator 1.702258e-01\n",
            "Losses after mini-batch   100: generator 5.169985e+00, discriminator 2.883309e-02\n",
            "Losses after mini-batch   150: generator 5.345879e+00, discriminator 1.480147e-02\n",
            "Losses after mini-batch   200: generator 5.597836e+00, discriminator 9.365019e-03\n",
            "Losses after mini-batch   250: generator 5.661310e+00, discriminator 1.040427e-02\n",
            "Losses after mini-batch   300: generator 6.007768e+00, discriminator 6.910379e-03\n",
            "Losses after mini-batch   350: generator 6.140148e+00, discriminator 5.780021e-03\n",
            "Losses after mini-batch   400: generator 3.793960e+00, discriminator 1.249252e-01\n",
            "Losses after mini-batch   450: generator 4.490350e+00, discriminator 6.518793e-02\n",
            "Starting epoch 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 3.897274e+00, discriminator 1.106836e-01\n",
            "Losses after mini-batch    50: generator 3.395404e+00, discriminator 2.630955e-01\n",
            "Losses after mini-batch   100: generator 3.297282e+00, discriminator 2.127976e-01\n",
            "Losses after mini-batch   150: generator 5.572125e-01, discriminator 1.491062e+00\n",
            "Losses after mini-batch   200: generator 3.024915e+00, discriminator 1.883266e-01\n",
            "Losses after mini-batch   250: generator 3.826888e+00, discriminator 2.076707e-01\n",
            "Losses after mini-batch   300: generator 4.014043e+00, discriminator 2.380215e-01\n",
            "Losses after mini-batch   350: generator 2.314554e+00, discriminator 3.636157e-01\n",
            "Losses after mini-batch   400: generator 3.854733e+00, discriminator 2.659187e-01\n",
            "Losses after mini-batch   450: generator 3.713568e+00, discriminator 4.253619e-01\n",
            "Starting epoch 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 4.685755e+00, discriminator 5.545884e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-7f997d39b147>:7: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure(figsize=(10, 10))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch    50: generator 3.306254e+00, discriminator 3.003380e-01\n",
            "Losses after mini-batch   100: generator 2.631975e+00, discriminator 4.298321e-01\n",
            "Losses after mini-batch   150: generator 1.732402e+00, discriminator 4.613539e-01\n",
            "Losses after mini-batch   200: generator 1.751425e+00, discriminator 5.420253e-01\n",
            "Losses after mini-batch   250: generator 2.920355e+00, discriminator 4.556205e-01\n",
            "Losses after mini-batch   300: generator 2.016718e+00, discriminator 4.717377e-01\n",
            "Losses after mini-batch   350: generator 1.602491e+00, discriminator 5.622091e-01\n",
            "Losses after mini-batch   400: generator 1.954332e+00, discriminator 4.665824e-01\n",
            "Losses after mini-batch   450: generator 1.806394e+00, discriminator 2.354738e+00\n",
            "Starting epoch 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 2.231366e+00, discriminator 4.967058e-01\n",
            "Losses after mini-batch    50: generator 2.674438e+00, discriminator 6.179941e-01\n",
            "Losses after mini-batch   100: generator 2.585378e+00, discriminator 5.646242e-01\n",
            "Losses after mini-batch   150: generator 3.510621e+00, discriminator 4.661982e-01\n",
            "Losses after mini-batch   200: generator 2.213548e+00, discriminator 5.141616e-01\n",
            "Losses after mini-batch   250: generator 1.861181e+00, discriminator 7.445556e-01\n",
            "Losses after mini-batch   300: generator 1.806995e+00, discriminator 4.521441e-01\n",
            "Losses after mini-batch   350: generator 1.909396e+00, discriminator 4.880114e-01\n",
            "Losses after mini-batch   400: generator 2.240635e+00, discriminator 5.624152e-01\n",
            "Losses after mini-batch   450: generator 2.400492e+00, discriminator 4.320912e-01\n",
            "Starting epoch 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.674424e+00, discriminator 7.224095e-01\n",
            "Losses after mini-batch    50: generator 2.134366e+00, discriminator 4.960675e-01\n",
            "Losses after mini-batch   100: generator 4.920460e+00, discriminator 2.425767e+00\n",
            "Losses after mini-batch   150: generator 2.025919e+00, discriminator 3.292936e-01\n",
            "Losses after mini-batch   200: generator 2.281728e+00, discriminator 3.316263e-01\n",
            "Losses after mini-batch   250: generator 1.835874e+00, discriminator 4.802105e-01\n",
            "Losses after mini-batch   300: generator 1.522792e+00, discriminator 6.971202e-01\n",
            "Losses after mini-batch   350: generator 2.099261e+00, discriminator 5.181534e-01\n",
            "Losses after mini-batch   400: generator 1.404960e+00, discriminator 6.312887e-01\n",
            "Losses after mini-batch   450: generator 1.474035e+00, discriminator 5.915858e-01\n",
            "Starting epoch 5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 3.073858e+00, discriminator 5.975434e-01\n",
            "Losses after mini-batch    50: generator 1.279253e+00, discriminator 7.870586e-01\n",
            "Losses after mini-batch   100: generator 1.334743e+00, discriminator 6.975995e-01\n",
            "Losses after mini-batch   150: generator 2.101851e+00, discriminator 1.031355e+00\n",
            "Losses after mini-batch   200: generator 2.283996e+00, discriminator 3.972930e-01\n",
            "Losses after mini-batch   250: generator 2.047262e+00, discriminator 4.053554e-01\n",
            "Losses after mini-batch   300: generator 2.026505e+00, discriminator 6.711549e-01\n",
            "Losses after mini-batch   350: generator 1.860485e+00, discriminator 5.027138e-01\n",
            "Losses after mini-batch   400: generator 1.786923e+00, discriminator 8.829906e-01\n",
            "Losses after mini-batch   450: generator 1.530796e+00, discriminator 5.930409e-01\n",
            "Starting epoch 6...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.935288e+00, discriminator 4.930502e-01\n",
            "Losses after mini-batch    50: generator 2.843404e+00, discriminator 8.636608e-01\n",
            "Losses after mini-batch   100: generator 2.081995e+00, discriminator 9.305018e-01\n",
            "Losses after mini-batch   150: generator 1.468956e+00, discriminator 8.042488e-01\n",
            "Losses after mini-batch   200: generator 1.724006e+00, discriminator 6.486202e-01\n",
            "Losses after mini-batch   250: generator 1.667687e+00, discriminator 9.325395e-01\n",
            "Losses after mini-batch   300: generator 2.598744e+00, discriminator 9.075179e-01\n",
            "Losses after mini-batch   350: generator 1.747244e+00, discriminator 8.227051e-01\n",
            "Losses after mini-batch   400: generator 1.529133e+00, discriminator 9.037035e-01\n",
            "Losses after mini-batch   450: generator 3.565210e-01, discriminator 1.263305e+00\n",
            "Starting epoch 7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.446904e+00, discriminator 7.312565e-01\n",
            "Losses after mini-batch    50: generator 3.081560e+00, discriminator 9.391755e-01\n",
            "Losses after mini-batch   100: generator 1.572056e+00, discriminator 8.466035e-01\n",
            "Losses after mini-batch   150: generator 1.333166e+00, discriminator 8.005737e-01\n",
            "Losses after mini-batch   200: generator 1.425069e+00, discriminator 7.227771e-01\n",
            "Losses after mini-batch   250: generator 1.539525e+00, discriminator 6.979072e-01\n",
            "Losses after mini-batch   300: generator 3.638102e+00, discriminator 7.025957e-01\n",
            "Losses after mini-batch   350: generator 3.793631e+00, discriminator 1.130461e+00\n",
            "Losses after mini-batch   400: generator 3.270774e+00, discriminator 6.985210e-01\n",
            "Losses after mini-batch   450: generator 1.739114e+00, discriminator 7.857564e-01\n",
            "Starting epoch 8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.094548e+00, discriminator 7.779018e-01\n",
            "Losses after mini-batch    50: generator 1.970034e+00, discriminator 6.230409e-01\n",
            "Losses after mini-batch   100: generator 3.192351e+00, discriminator 9.224123e-01\n",
            "Losses after mini-batch   150: generator 1.206715e+00, discriminator 8.634254e-01\n",
            "Losses after mini-batch   200: generator 1.701605e+00, discriminator 8.191263e-01\n",
            "Losses after mini-batch   250: generator 1.275862e+00, discriminator 8.414352e-01\n",
            "Losses after mini-batch   300: generator 2.859913e+00, discriminator 7.815036e-01\n",
            "Losses after mini-batch   350: generator 1.201733e+00, discriminator 8.189035e-01\n",
            "Losses after mini-batch   400: generator 1.244664e+00, discriminator 9.439195e-01\n",
            "Losses after mini-batch   450: generator 1.289352e+00, discriminator 6.369122e-01\n",
            "Starting epoch 9...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.536433e+00, discriminator 5.945982e-01\n",
            "Losses after mini-batch    50: generator 1.903441e+00, discriminator 5.703248e-01\n",
            "Losses after mini-batch   100: generator 2.574678e+00, discriminator 7.538894e-01\n",
            "Losses after mini-batch   150: generator 1.212264e+00, discriminator 9.455264e-01\n",
            "Losses after mini-batch   200: generator 2.893515e+00, discriminator 8.942097e-01\n",
            "Losses after mini-batch   250: generator 1.037195e+00, discriminator 1.025690e+00\n",
            "Losses after mini-batch   300: generator 1.894635e+00, discriminator 9.794149e-01\n",
            "Losses after mini-batch   350: generator 2.148865e+00, discriminator 8.285104e-01\n",
            "Losses after mini-batch   400: generator 2.423038e+00, discriminator 8.856530e-01\n",
            "Losses after mini-batch   450: generator 1.168539e+00, discriminator 8.433081e-01\n",
            "Starting epoch 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.432928e+00, discriminator 8.472264e-01\n",
            "Losses after mini-batch    50: generator 1.236770e+00, discriminator 8.128613e-01\n",
            "Losses after mini-batch   100: generator 2.408355e+00, discriminator 8.692333e-01\n",
            "Losses after mini-batch   150: generator 1.782498e+00, discriminator 6.805961e-01\n",
            "Losses after mini-batch   200: generator 1.283760e+00, discriminator 8.060550e-01\n",
            "Losses after mini-batch   250: generator 2.350481e+00, discriminator 9.639276e-01\n",
            "Losses after mini-batch   300: generator 9.891164e-01, discriminator 1.279297e+00\n",
            "Losses after mini-batch   350: generator 1.543710e+00, discriminator 8.890617e-01\n",
            "Losses after mini-batch   400: generator 1.803145e+00, discriminator 7.214101e-01\n",
            "Losses after mini-batch   450: generator 1.742857e+00, discriminator 7.652397e-01\n",
            "Starting epoch 11...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 2.404375e+00, discriminator 9.118286e-01\n",
            "Losses after mini-batch    50: generator 2.142672e+00, discriminator 1.011166e+00\n",
            "Losses after mini-batch   100: generator 1.357652e+00, discriminator 8.618417e-01\n",
            "Losses after mini-batch   150: generator 1.432043e+00, discriminator 7.753581e-01\n",
            "Losses after mini-batch   200: generator 1.232443e+00, discriminator 8.034535e-01\n",
            "Losses after mini-batch   250: generator 1.884487e+00, discriminator 1.016623e+00\n",
            "Losses after mini-batch   300: generator 2.238004e+00, discriminator 7.989124e-01\n",
            "Losses after mini-batch   350: generator 1.784765e+00, discriminator 9.530393e-01\n",
            "Losses after mini-batch   400: generator 1.114980e+00, discriminator 8.874659e-01\n",
            "Losses after mini-batch   450: generator 1.345373e+00, discriminator 1.005809e+00\n",
            "Starting epoch 12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.465342e+00, discriminator 9.761033e-01\n",
            "Losses after mini-batch    50: generator 1.852393e+00, discriminator 7.720472e-01\n",
            "Losses after mini-batch   100: generator 1.525997e+00, discriminator 8.272235e-01\n",
            "Losses after mini-batch   150: generator 1.844138e+00, discriminator 7.760232e-01\n",
            "Losses after mini-batch   200: generator 1.014630e+00, discriminator 8.821792e-01\n",
            "Losses after mini-batch   250: generator 1.719439e+00, discriminator 9.090003e-01\n",
            "Losses after mini-batch   300: generator 1.754416e+00, discriminator 7.488331e-01\n",
            "Losses after mini-batch   350: generator 1.879626e+00, discriminator 9.187724e-01\n",
            "Losses after mini-batch   400: generator 1.883034e+00, discriminator 8.805267e-01\n",
            "Losses after mini-batch   450: generator 9.281010e-01, discriminator 1.097738e+00\n",
            "Starting epoch 13...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.212918e+00, discriminator 1.044577e+00\n",
            "Losses after mini-batch    50: generator 2.311161e+00, discriminator 1.113393e+00\n",
            "Losses after mini-batch   100: generator 1.561879e+00, discriminator 9.895273e-01\n",
            "Losses after mini-batch   150: generator 1.747500e+00, discriminator 9.280275e-01\n",
            "Losses after mini-batch   200: generator 9.538560e-01, discriminator 1.122527e+00\n",
            "Losses after mini-batch   250: generator 1.517029e+00, discriminator 9.991222e-01\n",
            "Losses after mini-batch   300: generator 2.045583e+00, discriminator 6.402407e-01\n",
            "Losses after mini-batch   350: generator 8.014965e-01, discriminator 1.108738e+00\n",
            "Losses after mini-batch   400: generator 7.537541e-01, discriminator 1.000954e+00\n",
            "Losses after mini-batch   450: generator 2.124397e+00, discriminator 1.191835e+00\n",
            "Starting epoch 14...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 2.126314e+00, discriminator 1.134032e+00\n",
            "Losses after mini-batch    50: generator 1.116367e+00, discriminator 9.645762e-01\n",
            "Losses after mini-batch   100: generator 9.197205e-01, discriminator 1.227804e+00\n",
            "Losses after mini-batch   150: generator 1.001060e+00, discriminator 1.033638e+00\n",
            "Losses after mini-batch   200: generator 2.458090e+00, discriminator 1.298786e+00\n",
            "Losses after mini-batch   250: generator 1.400761e+00, discriminator 9.652902e-01\n",
            "Losses after mini-batch   300: generator 2.688741e+00, discriminator 1.322294e+00\n",
            "Losses after mini-batch   350: generator 1.956671e+00, discriminator 8.845258e-01\n",
            "Losses after mini-batch   400: generator 1.769895e+00, discriminator 8.658626e-01\n",
            "Losses after mini-batch   450: generator 1.723001e+00, discriminator 8.307851e-01\n",
            "Starting epoch 15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.790387e+00, discriminator 7.990220e-01\n",
            "Losses after mini-batch    50: generator 1.281504e+00, discriminator 9.356538e-01\n",
            "Losses after mini-batch   100: generator 1.791236e+00, discriminator 9.223534e-01\n",
            "Losses after mini-batch   150: generator 1.525257e+00, discriminator 1.006165e+00\n",
            "Losses after mini-batch   200: generator 1.929530e+00, discriminator 7.631478e-01\n",
            "Losses after mini-batch   250: generator 1.651028e+00, discriminator 9.618432e-01\n",
            "Losses after mini-batch   300: generator 1.646595e+00, discriminator 1.053203e+00\n",
            "Losses after mini-batch   350: generator 1.695013e+00, discriminator 1.013471e+00\n",
            "Losses after mini-batch   400: generator 1.787546e+00, discriminator 9.250385e-01\n",
            "Losses after mini-batch   450: generator 1.576933e+00, discriminator 1.178345e+00\n",
            "Starting epoch 16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.170830e+00, discriminator 1.093097e+00\n",
            "Losses after mini-batch    50: generator 1.458102e+00, discriminator 1.032808e+00\n",
            "Losses after mini-batch   100: generator 1.581947e+00, discriminator 9.144628e-01\n",
            "Losses after mini-batch   150: generator 1.824137e+00, discriminator 9.419660e-01\n",
            "Losses after mini-batch   200: generator 1.365532e+00, discriminator 9.240114e-01\n",
            "Losses after mini-batch   250: generator 1.881389e+00, discriminator 9.359596e-01\n",
            "Losses after mini-batch   300: generator 2.134047e+00, discriminator 4.620452e-01\n",
            "Losses after mini-batch   350: generator 1.033328e+00, discriminator 1.273652e+00\n",
            "Losses after mini-batch   400: generator 1.887329e+00, discriminator 1.071402e+00\n",
            "Losses after mini-batch   450: generator 1.690945e+00, discriminator 1.075788e+00\n",
            "Starting epoch 17...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.163267e+00, discriminator 1.088864e+00\n",
            "Losses after mini-batch    50: generator 1.427166e+00, discriminator 1.066024e+00\n",
            "Losses after mini-batch   100: generator 1.350350e+00, discriminator 1.122705e+00\n",
            "Losses after mini-batch   150: generator 2.025908e+00, discriminator 1.199056e+00\n",
            "Losses after mini-batch   200: generator 8.614424e-01, discriminator 1.192183e+00\n",
            "Losses after mini-batch   250: generator 1.710616e+00, discriminator 1.017745e+00\n",
            "Losses after mini-batch   300: generator 1.246889e+00, discriminator 1.073515e+00\n",
            "Losses after mini-batch   350: generator 1.194686e+00, discriminator 9.609898e-01\n",
            "Losses after mini-batch   400: generator 1.414427e+00, discriminator 8.933073e-01\n",
            "Losses after mini-batch   450: generator 8.119959e-01, discriminator 1.277390e+00\n",
            "Starting epoch 18...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 2.003576e+00, discriminator 1.060094e+00\n",
            "Losses after mini-batch    50: generator 2.013410e+00, discriminator 1.184777e+00\n",
            "Losses after mini-batch   100: generator 1.646078e+00, discriminator 1.052470e+00\n",
            "Losses after mini-batch   150: generator 1.353816e+00, discriminator 1.112806e+00\n",
            "Losses after mini-batch   200: generator 9.694455e-01, discriminator 1.218591e+00\n",
            "Losses after mini-batch   250: generator 1.318242e+00, discriminator 1.008714e+00\n",
            "Losses after mini-batch   300: generator 1.096961e+00, discriminator 1.064635e+00\n",
            "Losses after mini-batch   350: generator 1.409432e+00, discriminator 1.436710e+00\n",
            "Losses after mini-batch   400: generator 1.299627e+00, discriminator 1.030300e+00\n",
            "Losses after mini-batch   450: generator 1.332623e+00, discriminator 1.106854e+00\n",
            "Starting epoch 19...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 2.608204e+00, discriminator 1.307840e+00\n",
            "Losses after mini-batch    50: generator 2.095638e+00, discriminator 1.328143e+00\n",
            "Losses after mini-batch   100: generator 1.567434e+00, discriminator 1.321671e+00\n",
            "Losses after mini-batch   150: generator 1.422762e+00, discriminator 8.411960e-01\n",
            "Losses after mini-batch   200: generator 7.545482e-01, discriminator 1.294925e+00\n",
            "Losses after mini-batch   250: generator 1.229625e+00, discriminator 1.109365e+00\n",
            "Losses after mini-batch   300: generator 1.678848e+00, discriminator 1.039071e+00\n",
            "Losses after mini-batch   350: generator 1.122537e+00, discriminator 1.213960e+00\n",
            "Losses after mini-batch   400: generator 1.433805e+00, discriminator 1.262551e+00\n",
            "Losses after mini-batch   450: generator 2.641670e+00, discriminator 8.361417e-01\n",
            "Starting epoch 20...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.048425e+00, discriminator 1.078836e+00\n",
            "Losses after mini-batch    50: generator 1.680444e+00, discriminator 1.249389e+00\n",
            "Losses after mini-batch   100: generator 2.000468e+00, discriminator 1.431161e+00\n",
            "Losses after mini-batch   150: generator 1.210974e+00, discriminator 1.075807e+00\n",
            "Losses after mini-batch   200: generator 1.902345e+00, discriminator 1.265608e+00\n",
            "Losses after mini-batch   250: generator 1.341498e+00, discriminator 1.248152e+00\n",
            "Losses after mini-batch   300: generator 2.152675e+00, discriminator 1.061451e+00\n",
            "Losses after mini-batch   350: generator 7.490135e-01, discriminator 1.209101e+00\n",
            "Losses after mini-batch   400: generator 1.007142e+00, discriminator 1.065563e+00\n",
            "Losses after mini-batch   450: generator 1.635502e+00, discriminator 1.248465e+00\n",
            "Starting epoch 21...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.083246e+00, discriminator 1.173137e+00\n",
            "Losses after mini-batch    50: generator 1.166541e+00, discriminator 1.045266e+00\n",
            "Losses after mini-batch   100: generator 1.634998e+00, discriminator 1.001598e+00\n",
            "Losses after mini-batch   150: generator 1.341503e+00, discriminator 1.115064e+00\n",
            "Losses after mini-batch   200: generator 1.771170e+00, discriminator 1.315347e+00\n",
            "Losses after mini-batch   250: generator 1.137290e+00, discriminator 1.080409e+00\n",
            "Losses after mini-batch   300: generator 1.251991e+00, discriminator 1.126464e+00\n",
            "Losses after mini-batch   350: generator 1.905983e+00, discriminator 1.230794e+00\n",
            "Losses after mini-batch   400: generator 1.307745e+00, discriminator 1.142025e+00\n",
            "Losses after mini-batch   450: generator 1.446775e+00, discriminator 1.350962e+00\n",
            "Starting epoch 22...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.216551e+00, discriminator 1.159139e+00\n",
            "Losses after mini-batch    50: generator 1.048758e+00, discriminator 1.148065e+00\n",
            "Losses after mini-batch   100: generator 1.619911e+00, discriminator 1.062228e+00\n",
            "Losses after mini-batch   150: generator 9.936600e-01, discriminator 1.202679e+00\n",
            "Losses after mini-batch   200: generator 1.224287e+00, discriminator 1.245801e+00\n",
            "Losses after mini-batch   250: generator 1.338113e+00, discriminator 7.732950e-01\n",
            "Losses after mini-batch   300: generator 1.285918e+00, discriminator 1.402481e+00\n",
            "Losses after mini-batch   350: generator 1.493184e+00, discriminator 1.062852e+00\n",
            "Losses after mini-batch   400: generator 8.929476e-01, discriminator 1.143063e+00\n",
            "Losses after mini-batch   450: generator 1.361794e+00, discriminator 1.089608e+00\n",
            "Starting epoch 23...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.784431e+00, discriminator 1.165457e+00\n",
            "Losses after mini-batch    50: generator 1.833252e+00, discriminator 1.175720e+00\n",
            "Losses after mini-batch   100: generator 1.234771e+00, discriminator 1.228319e+00\n",
            "Losses after mini-batch   150: generator 7.347151e-01, discriminator 1.368850e+00\n",
            "Losses after mini-batch   200: generator 1.173841e+00, discriminator 1.147368e+00\n",
            "Losses after mini-batch   250: generator 1.825684e+00, discriminator 1.046253e+00\n",
            "Losses after mini-batch   300: generator 1.562614e+00, discriminator 1.302346e+00\n",
            "Losses after mini-batch   350: generator 9.352173e-01, discriminator 1.097632e+00\n",
            "Losses after mini-batch   400: generator 1.047373e+00, discriminator 1.116732e+00\n",
            "Losses after mini-batch   450: generator 1.040196e+00, discriminator 1.241716e+00\n",
            "Starting epoch 24...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.359088e+00, discriminator 1.142778e+00\n",
            "Losses after mini-batch    50: generator 1.513069e+00, discriminator 1.037774e+00\n",
            "Losses after mini-batch   100: generator 1.559174e+00, discriminator 1.296668e+00\n",
            "Losses after mini-batch   150: generator 1.212633e+00, discriminator 1.138868e+00\n",
            "Losses after mini-batch   200: generator 1.547891e+00, discriminator 1.150158e+00\n",
            "Losses after mini-batch   250: generator 9.601591e-01, discriminator 1.158287e+00\n",
            "Losses after mini-batch   300: generator 1.356546e+00, discriminator 1.276841e+00\n",
            "Losses after mini-batch   350: generator 1.428357e+00, discriminator 1.151460e+00\n",
            "Losses after mini-batch   400: generator 1.053181e+00, discriminator 1.266276e+00\n",
            "Losses after mini-batch   450: generator 1.070544e+00, discriminator 1.178359e+00\n",
            "Starting epoch 25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 1.945721e+00, discriminator 1.128907e+00\n",
            "Losses after mini-batch    50: generator 9.510391e-01, discriminator 1.320748e+00\n",
            "Losses after mini-batch   100: generator 1.116756e+00, discriminator 1.149484e+00\n",
            "Losses after mini-batch   150: generator 1.295155e+00, discriminator 1.174234e+00\n",
            "Losses after mini-batch   200: generator 1.464157e+00, discriminator 1.170699e+00\n",
            "Losses after mini-batch   250: generator 1.174613e+00, discriminator 1.077940e+00\n",
            "Losses after mini-batch   300: generator 1.177768e+00, discriminator 1.177479e+00\n",
            "Losses after mini-batch   350: generator 1.222189e+00, discriminator 1.071556e+00\n",
            "Losses after mini-batch   400: generator 1.482883e+00, discriminator 1.450155e+00\n",
            "Losses after mini-batch   450: generator 1.258314e+00, discriminator 1.184183e+00\n",
            "Starting epoch 26...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch     0: generator 2.041207e+00, discriminator 1.177991e+00\n",
            "Losses after mini-batch    50: generator 1.413564e+00, discriminator 1.195478e+00\n",
            "Losses after mini-batch   100: generator 1.556129e+00, discriminator 1.269714e+00\n"
          ]
        }
      ]
    }
  ]
}